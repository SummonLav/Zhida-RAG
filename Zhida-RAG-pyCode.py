# %% [markdown]
# # Build a Retrieval Augmented Generation (RAG) Model for Zhida Technology
# 
# Document from æ¸¯äº¤æ‰€ä¸Šå¸‚æ–‡ä»¶-æŒšè¾¾ç§‘æŠ€-ä¸šåŠ¡ã€https://www1.hkexnews.hk/app/sehk/2024/106935/2024112801952_c.htmlã€‘
# 
# Reference Links to Learn LangChain:
# 1. DeepLearning.ai Course: https://learn.deeplearning.ai/courses/langchain-chat-with-your-data
# 2. LangChain Github: https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials

# %% [markdown]
# ## Retrieval augmented generation
#  
# In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution. 
# 
# This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc).

# %% [markdown]
# ### Document Loading
# 
# Reference: DeepLearning.ai -> https://learn.deeplearning.ai/courses/langchain-chat-with-your-data/lesson/2/document-loading
# 
# Document Reference: 01_document_loading.ipynb

# %%
import os
import getpass
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
openai.api_key  = os.environ['OPENAI_API_KEY']

# %% [markdown]
# Load PDF (*PyPDFLoader* by *langchain.document_loaders* does NOT support PDFs from æ¸¯äº¤æ‰€)
# 
# Way 1: pdfplumber
# 

# %%
#%pip install pypdf

# %%
from langchain.document_loaders import PyPDFLoader
import pdfplumber

loader = pdfplumber.open("/Users/lavendashan/Documents/AIML/LangChain-RAG/Files/æŒšè¾¾-ä¸šåŠ¡.pdf")
pages = loader.pages

# ï¼è¯»å–ä¸ºä¹±ç  ï¼#
#loader = PyPDFLoader("/Users/lavendashan/Documents/AIML/LangChain-RAG/Files/æŒšè¾¾-ä¸šåŠ¡.pdf")
#pages = loader.load()

### TEST ###
#import pdfplumber
#with pdfplumber.open("/Users/lavendashan/Documents/AIML/LangChain-RAG/Files/æŒšè¾¾-ä¸šåŠ¡.pdf") as pdf:
 #   for page in pdf.pages:
  #      text = page.extract_text()
   #     print(text)

# %%
len(pages)

# %%
### ç¬¬ä¸€é¡µ ###
page = pages[0]

print(page.extract_text())

# %% [markdown]
# Way 2: LangChain PDFPlumberLoader
# 
# Reference: https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/

# %%
from langchain_community.document_loaders import PDFPlumberLoader

loader = PDFPlumberLoader("/Users/lavendashan/Documents/AIML/LangChain-RAG/Files/æŒšè¾¾-ä¸šåŠ¡.pdf")
docs = loader.load()

# %%
print(docs[0])

# %%
### å›¾ç‰‡åˆ†æä¸äº†ï¼Œåªèƒ½è¯»å–é‡Œé¢çš„æ–‡æœ¬ ###
print(docs[3])

# %%
### æ£€æŸ¥è¡¨æ ¼è¯»å–ï¼Œåº”è¯¥å¯ä»¥ ###
print(docs[19])

# %%
# æå–æ‰€æœ‰æ–‡æ¡£çš„æ–‡æœ¬å†…å®¹
all_text = " ".join([doc.page_content for doc in docs])
print(all_text[:500])  # æ‰“å°å‰500ä¸ªå­—ç¬¦ä½œä¸ºç¤ºä¾‹

# %%
# å»æ‰ç©ºæ ¼ï¼Œä¿ç•™æ¢è¡Œç¬¦
all_text = all_text.replace(" ", "")

print(all_text[:500])

# %% [markdown]
# ### Embedding and VectorStore

# %% [markdown]
# Use Way 2: *LangChain PDFPlumberLoader* to continue

# %% [markdown]
# é€šå¸¸ï¼Œ**Embedding** ç”Ÿæˆå‘é‡ï¼Œ**VectorStore** å­˜å‚¨å‘é‡å¹¶é«˜æ•ˆæ£€ç´¢ç›¸å…³å†…å®¹ã€‚
# 
# **Embedding** å’Œ**Vectorize** åŒºåˆ«ï¼š<br>
# **Embedding** æ›´æ³¨é‡è¯­ä¹‰ï¼Œæ˜¯ä¸€ç§æ·±å±‚çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å‘é‡åŒ–æ–¹æ³•ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰æœç´¢ã€æ¨èç³»ç»Ÿï¼‰ã€‚<br>
# **Vectorize** æ˜¯æ›´é€šç”¨çš„å‘é‡åŒ–æŠ€æœ¯ï¼Œå¯ä»¥æ˜¯ç®€å•ç»Ÿè®¡ã€ç‰¹å¾æå–ï¼Œæˆ–è€…æ·±åº¦å­¦ä¹ ç”Ÿæˆçš„å‘é‡ã€‚

# %% [markdown]
# åˆæ˜¯æŠ¥é”™ -- LangChainå®˜æ–¹çš„text_splitterä¸èƒ½åˆ†å‰²ä¸­æ–‡ğŸ˜…ï¼Œé‚£å°±è¯•è¯•jiebaï¼

# %% [markdown]
# **RecursiveCharacterTextSplitter** vs. **Jieba** åŒºåˆ« <br>
# **RecursiveCharacterTextSplitter**:
# - æŒ‰æ®µè½ã€å¥å­æˆ–å›ºå®šé•¿åº¦çš„å—è¿›è¡Œåˆ†å‰²ã€‚
# - ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼Œé€šå¸¸åˆ†å‰²ç»“æœæ˜¯è¾ƒå¤§çš„æ–‡æœ¬å—ï¼ˆå¥å­å—æˆ–å­—ç¬¦å—ï¼‰ï¼Œä¸»è¦ç”¨äºç”Ÿæˆè¯­è¨€æ¨¡å‹çš„è¾“å…¥ç‰‡æ®µã€‚
# 
# **Jieba**:
# - æŒ‰å•è¯ç²’åº¦è¿›è¡Œåˆ†è¯ï¼Œå°†å¥å­åˆ‡å‰²æˆæœ€å°çš„è¯æ±‡å•ä½ã€‚
# - ä¸»è¦ç”¨äºè¯­ä¹‰åˆ†æã€ä¿¡æ¯æ£€ç´¢ç­‰éœ€è¦ç²¾å‡†è¯­ä¹‰å•ä½çš„åœºæ™¯ã€‚

# %%
import jieba

seg_list = jieba.cut(all_text, cut_all=False)
#print("Default Mode: " + "/ ".join(seg_list))  # ç²¾ç¡®æ¨¡å¼

# %% [markdown]
# æ„Ÿè§‰åˆ†è¯ç»“æœå¤ªç»†äº†ï¼Œè¯•è¯•ç±»LangChain RecursiveCharacterTextSplitterçš„æŒ‰å›ºå®šé•¿åº¦åˆ†å—çœ‹çœ‹æ¨¡å‹ä¸Šä¸‹æ–‡ç†è§£çš„æ•ˆæœğŸ‘€ï¼Ÿ<br>
# ï¼ˆchunk_size å’Œ chunk_overlap ä¸ LangChain å‚è€ƒæ–‡æ¡£ç›¸åŒï¼‰
# 
# **å¯èƒ½é—®é¢˜**ï¼š
# åˆ†è¯ç»“æœå¤ªç»†ä¼šå¯¼è‡´å‘é‡æ•°æ®åº“å­˜å‚¨å¤§é‡ç¢ç‰‡ï¼Œå½±å“æ£€ç´¢æ•ˆæœå’Œç”Ÿæˆçš„ä¸Šä¸‹æ–‡è´¨é‡ã€‚<br>
# **è§£å†³æ–¹æ¡ˆ**ï¼š
# å°†åˆ†è¯åçš„ç»“æœé‡æ–°ç»„ç»‡ä¸ºæ›´å¤§çš„å—ï¼ˆchunkï¼‰ï¼Œä¾‹å¦‚ä»¥ä¸€å®šçš„å­—æ•°æˆ–å¥å­æ•°ä¸ºå•ä½è¿›è¡Œåˆå¹¶ã€‚
# 
# 

# %%
chunk_size = 1500  # æ¯å—æœ€å¤šåŒ…å« 1500 ä¸ªå­—ç¬¦
chunk_overlap = 150

# å®ç°åˆ†å—å¹¶åŠ å…¥é‡å 
chunks = [
    all_text[i:i + chunk_size]
    for i in range(0, len(all_text), chunk_size - chunk_overlap)
    if i < len(all_text)  # ç¡®ä¿å—èµ·å§‹ç´¢å¼•åœ¨æ–‡æœ¬èŒƒå›´å†…
]
print("æŒ‰å›ºå®šé•¿åº¦åˆ†å—ï¼ˆå«é‡å ï¼‰:", chunks)

# å¯¹æ¯ä¸ªå—è¿›è¡Œåˆ†è¯
segmented_chunks = [" ".join(jieba.cut(chunk)) for chunk in chunks]
print("åˆ†è¯åï¼ˆå«é‡å ï¼‰:", segmented_chunks)

# %%
len(segmented_chunks[0])   # æ£€æŸ¥ chunk_size

# %% [markdown]
# å°† segmented_chunks è½¬ä¸º LangChain æ–‡æ¡£å¯¹è±¡ï¼Œç»§ç»­å‘é‡å­˜å‚¨å’Œæ£€ç´¢çš„æ­¥éª¤

# %%
from langchain.schema import Document

# å°†åˆ†è¯åçš„æ–‡æœ¬å—è½¬æ¢ä¸º LangChain æ–‡æ¡£å¯¹è±¡
splits = [Document(page_content=chunk) for chunk in segmented_chunks]
print("LangChain æ–‡æ¡£å¯¹è±¡:", splits[:2])

# %%
print("LangChain æ–‡æ¡£å¯¹è±¡:", splits[-1])

# %%
print(f"åˆ›å»ºäº† {len(splits)} ä¸ª LangChain æ–‡æ¡£ã€‚")

# %% [markdown]
# å°†è¿™äº›åˆå¹¶åçš„æ–‡æ¡£å—å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­ï¼Œæ–¹ä¾¿åç»­æ£€ç´¢

# %%
#!rm -rf /Users/lavendashan/Documents/AIML/LangChain-RAG/docs  # remove old database files if any

# %%
#!mkdir -p /Users/lavendashan/Documents/AIML/LangChain-RAG/docs/chroma

# %%
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings

persist_directory = "/Users/lavendashan/Documents/AIML/LangChain-RAG/docs/chroma"
embedding = OpenAIEmbeddings()

# %%
# å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
vectordb = Chroma.from_documents(
    documents=splits, 
    embedding=embedding, 
    persist_directory=persist_directory
)

# æŒä¹…åŒ–å­˜å‚¨
vectordb.persist()
print("å‘é‡æ•°æ®åº“å·²å­˜å‚¨æˆåŠŸï¼")

# %%
print(vectordb._collection.count())

# %% [markdown]
# ### RAG-enabled ChatBot Interface

# %%
# åŠ è½½å­˜å‚¨çš„å‘é‡æ•°æ®åº“
retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# æµ‹è¯•æ£€ç´¢
query = "æŒšè¾¾ä¸šåŠ¡çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
results = retriever.get_relevant_documents(query)
for doc in results:
    print(doc.page_content)

# %%
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# ä½¿ç”¨ ChatOpenAI ä»£æ›¿ OpenAI
llm = ChatOpenAI(model="gpt-4o", temperature=1)

# åˆ›å»º RetrievalQA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm, 
    retriever=retriever, 
    chain_type="refine"
)

# %%
# æµ‹è¯•é—®ç­”
query = "è¯·é—®è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
answer = qa_chain.run(query)
print(f"å›ç­”: {answer}")

# %%
import gradio as gr

def chatbot(user_input):
    return qa_chain.run(user_input)

# Gradio Web èŠå¤©æœºå™¨äºº
iface = gr.Interface(
    fn=chatbot,
    inputs="text",
    outputs="text",
    title="æŒšè¾¾èŠå¤©æœºå™¨äºº"
)

iface.launch()

# %% [markdown]
# ### è¿›ä¸€æ­¥è°ƒè¯•

# %% [markdown]
# 1. æ— æ³•æ£€ç´¢åˆ°æœºå™¨äººç›¸å…³äº§å“ -- ã€è§£å†³æ–¹æ¡ˆã€‘GPT3.5æ›´æ–°åˆ°4o

# %%
# æµ‹è¯•æ£€ç´¢
query = "æŒšè¾¾ç§‘æŠ€ä¸æœºå™¨äººç›¸å…³çš„ä¸šåŠ¡æœ‰å“ªäº›ï¼Ÿ"
results = retriever.get_relevant_documents(query)
for doc in results:
    print(doc.page_content)

# %%
# æµ‹è¯•é—®ç­”
query = "æŒšè¾¾æœ‰å“ªäº›æœºå™¨äººäº§å“ï¼Ÿ"
answer = qa_chain.run(query)
print(f"å›ç­”: {answer}")

# %% [markdown]
# 2. émultimodal -- æ— æ³•è¯»å›¾

# %%
# æµ‹è¯•æ£€ç´¢
query = "æŒšè¾¾çš„APPåŒ…å«ä»€ä¹ˆå†…å®¹ï¼Ÿ"
results = retriever.get_relevant_documents(query)
for doc in results:
    print(doc.page_content)

# %%
# æµ‹è¯•é—®ç­”
query = "æŒšè¾¾çš„APPåŒ…å«ä»€ä¹ˆå†…å®¹ï¼Ÿ"
answer = qa_chain.run(query)
print(f"å›ç­”: {answer}")

# %% [markdown]
# 3. Hallucination (wrong answer) -- ã€è§£å†³æ–¹æ¡ˆã€‘åœ¨ qa_chain åŠ å…¥ chain_type="refine" çš„åˆå¹¶ç­”æ¡ˆæ–¹å¼ï¼Œè§£å†³äº†ä¹±å›ç­”çš„é—®é¢˜ä½†æ˜¯å›ç­”æ—¶é—´å˜é•¿ && ä¸èƒ½è‡ªåŠ¨è°ƒç”¨ ChatGPT çš„ RAG (Possible Reason: By default, most of LangChainâ€™s built-in RetrievalQA prompts encourage the LLM to say â€œI donâ€™t knowâ€ if the retrieval doesnâ€™t yield relevant context.)

# %%
# æµ‹è¯•é—®ç­”
query = "æŒšè¾¾ç§‘æŠ€æˆç«‹äºå“ªä¸€å¹´ï¼Ÿ"
results = retriever.get_relevant_documents(query)
for doc in results:
    print(doc.page_content)

# %%
# æµ‹è¯•é—®ç­”
query = "æŒšè¾¾ç§‘æŠ€æˆç«‹äºå“ªä¸€å¹´ï¼Ÿ"
answer = qa_chain.run(query)
print(f"å›ç­”: {answer}")

# %%


